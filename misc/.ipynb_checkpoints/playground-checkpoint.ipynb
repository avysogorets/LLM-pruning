{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d671980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6ccedf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataBase:\n",
    "    \"\"\" Data base class API: loads, preprocesses, and tokenizes datasets to \n",
    "        be ready for the BERT model.\n",
    "        Required attributes:\n",
    "         - num_classes: number of classes in a dataset\n",
    "         - datasets: dictionary with 'train' and 'val' tokenized datasets;\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.num_classes: int\n",
    "        self.datasets: Dict[str, Dataset] = {}  \n",
    "        \n",
    "        \n",
    "class IMDb(ClassificationDataBase):\n",
    "    def __init__(self, device, backbone_name, **kwargs):\n",
    "        super().__init__()\n",
    "        loading_kwargs = {'path': 'imdb', \n",
    "                          'train_filename': 'train.tsv',\n",
    "                          'dev_filename': 'dev.tsv',\n",
    "                          'header': 0,\n",
    "                          'index_col': 0}\n",
    "        self.device = device\n",
    "        self._prepare_datasets(loading_kwargs, device, backbone_name)\n",
    "    \n",
    "    def _prepare_datasets(self, loading_kwargs, device, backbone_name):\n",
    "        dataframes = load_dataframes(loading_kwargs)\n",
    "        self.num_classes = len(dataframes['train']['label'].unique())\n",
    "        data_X, data_y = {}, {}\n",
    "        for split in dataframes.keys():\n",
    "            data_X[split] = dataframes[split].drop('label', axis=1, inplace=False)\n",
    "            data_X[split] = data_X[split].values.tolist()\n",
    "            data_y[split] = dataframes[split]['label'].values\n",
    "            data_y[split] = torch.LongTensor(data_y[split]).to(device)\n",
    "            data_X[split] = get_tokens(data_X[split], backbone_name=backbone_name)\n",
    "            data_X[split] = {k: v.to(device) for k,v in data_X[split].items()}\n",
    "            self.datasets[split] = TokenizedDataset(data_X[split], data_y[split])\n",
    "        \n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "            \n",
    "    def __getitem__(self,idx):\n",
    "        return {key: self.X[key][idx] for key in self.X.keys()}, self.y[idx]\n",
    "\n",
    "\n",
    "def get_all_subclasses(cls):\n",
    "    all_subclasses = []\n",
    "    for subclass in cls.__subclasses__():\n",
    "        all_subclasses.append(subclass)\n",
    "        all_subclasses.extend(get_all_subclasses(subclass))\n",
    "    return all_subclasses\n",
    "\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def load_dataframes(loading_kwargs):\n",
    "    path = loading_kwargs['path']\n",
    "    train_filename = loading_kwargs['train_filename']\n",
    "    dev_filename = loading_kwargs['dev_filename']\n",
    "    header = loading_kwargs['header']\n",
    "    index_col = loading_kwargs['index_col']\n",
    "    assert '.' in train_filename, f\"unrecognized file format {train_filename}\"\n",
    "    extension = train_filename.split('.')[-1]\n",
    "    if extension == 'tsv':\n",
    "        delimiter = '\\t'\n",
    "    elif extension == 'csv':\n",
    "        delimiter = ','\n",
    "    else:\n",
    "        raise ValueError(f\"unrecognized file format {extension}\")\n",
    "    dataframes = {}\n",
    "    for split,split_filename in zip(['train','dev'], [train_filename, dev_filename]):\n",
    "        filename = os.path.join(path, split_filename)\n",
    "        dataframes[split] = pd.read_csv(\n",
    "                filename,\n",
    "                delimiter=delimiter,\n",
    "                header=header,\n",
    "                index_col=index_col,\n",
    "                engine=\"python\",\n",
    "                error_bad_lines=False,\n",
    "                warn_bad_lines=False)\n",
    "        new_columns = [f\"sentence_{i}\" for i in range(len(dataframes[split].columns)-1)]+[\"label\"]\n",
    "        dataframes[split].columns = new_columns\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def get_tokens(data_X: List[List[str]], backbone_name: str) -> dict:\n",
    "    tokenizer =  BertTokenizer.from_pretrained(backbone_name)\n",
    "    if len(data_X[0])==1:\n",
    "        data_X = [X[0] for X in data_X]\n",
    "    data_X = tokenizer.batch_encode_plus(\n",
    "                data_X,\n",
    "                add_special_tokens=True,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\")\n",
    "    return data_X\n",
    "    \n",
    "\n",
    "def stabilized_forward(linearized_encoder, min_, max_, scaler=1e3, max_attempts=10):\n",
    "    def forward(x, attention_mask=None):\n",
    "        for i in range(len(linearized_encoder.layer)):\n",
    "            attempts = 0\n",
    "            prelim_x = linearized_encoder.layer[i](x, attention_mask=attention_mask)[0]\n",
    "            while torch.mean(torch.abs(prelim_x))>max_ and attempts<max_attempts:\n",
    "                for m in linearized_encoder.layer[i].modules():\n",
    "                    if isinstance(m, torch.nn.Linear):\n",
    "                        m.weight.data = m.weight.data/scaler\n",
    "                    query = linearized_encoder.layer[i].attention.self.query.weight.data\n",
    "                    linearized_encoder.layer[i].attention.self.query.weight.data = scaler*query\n",
    "                    key = linearized_encoder.layer[i].attention.self.key.weight.data\n",
    "                    linearized_encoder.layer[i].attention.self.key.weight.data = scaler*key\n",
    "                prelim_x = linearized_encoder.layer[i](x, attention_mask=attention_mask)[0]\n",
    "                attempts+=1\n",
    "            while torch.mean(torch.abs(prelim_x))<min_ and attempts<max_attempts:\n",
    "                for m in linearized_encoder.layer[i].modules():\n",
    "                    if isinstance(m, torch.nn.Linear):\n",
    "                        m.weight.data = scaler*m.weight.data\n",
    "                    query = linearized_encoder.layer[i].attention.self.query.weight.data\n",
    "                    linearized_encoder.layer[i].attention.self.query.weight.data = query/scaler\n",
    "                    key = linearized_encoder.layer[i].attention.self.key.weight.data\n",
    "                    linearized_encoder.layer[i].attention.self.key.weight.data = key/scaler\n",
    "                prelim_x = linearized_encoder.layer[i](x, attention_mask=attention_mask)[0]\n",
    "                attempts+=1\n",
    "            assert attempts<max_attempts\n",
    "            x = prelim_x\n",
    "        return x\n",
    "    return forward\n",
    "\n",
    "\n",
    "def max_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded=(attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float())\n",
    "    token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "    return torch.max(token_embeddings, -2).values\n",
    "\n",
    "\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded=(attention_mask.unsqueeze(-1).float())\n",
    "    sum_embeddings=torch.sum(token_embeddings * input_mask_expanded, -2)\n",
    "    sum_mask=torch.clamp(input_mask_expanded.sum(-2), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def pool(x, encoded_input, pool_type):\n",
    "    if pool_type in ['avg', 'mean']:\n",
    "        x = mean_pooling(x['last_hidden_state'], encoded_input['attention_mask'])\n",
    "    elif pool_type == 'max':\n",
    "        x = max_pooling(x['last_hidden_state'], encoded_input['attention_mask'])\n",
    "    elif pool_type in ['cls', 'first']:\n",
    "        x = x['last_hidden_state'][:,0]\n",
    "    elif pool_type == 'pooler_output':\n",
    "        x = x['pooler_output']\n",
    "    else:\n",
    "        raise ValueError(f'unknown pool type {pool_type}.')\n",
    "    return x\n",
    "\n",
    "\n",
    "def effective_masks_dense(masks):\n",
    "    for i,mask in enumerate(masks):\n",
    "        assert len(mask.size())==2, f\"found mask of shape {mask.size()}\"\n",
    "        masks[i] = mask.T\n",
    "    units=[mask.shape[-2] for mask in masks]+[masks[-1].shape[-1]]\n",
    "    next_layer=torch.ones((units[-1],))\n",
    "    way_out=[next_layer]\n",
    "    for mask in masks[::-1]:\n",
    "        curr_mask=torch.matmul(mask,next_layer.view(len(next_layer),1))\n",
    "        next_layer=torch.sum(curr_mask,dim=1)>0\n",
    "        way_out.append(next_layer)\n",
    "    way_out=way_out[::-1]\n",
    "    prev_layer=torch.ones((units[0],))\n",
    "    way_in=[prev_layer]\n",
    "    for mask in masks:\n",
    "        curr_mask=torch.matmul(prev_layer.view(1,len(prev_layer)),mask)\n",
    "        prev_layer=torch.sum(curr_mask,dim=0)>0\n",
    "        way_in.append(prev_layer)\n",
    "    activity=[w_in*w_out for w_in,w_out in zip(way_in,way_out)]\n",
    "    effective_masks = []\n",
    "    for i,mask in enumerate(masks):\n",
    "        activity_prev = activity[i].view(len(activity[i]),1)\n",
    "        activity_next = activity[i+1].view(1,len(activity[i+1]))\n",
    "        effective_mask = mask*torch.matmul(activity_prev, activity_next)\n",
    "        effective_masks.append(effective_mask.T)\n",
    "    return effective_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0c8f5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModelBase(torch.nn.Module):\n",
    "    \"\"\" The base model API\n",
    "        Required attributes:\n",
    "         - masks: list of binary masks to be applied during forward pass;\n",
    "         - prunable_modules: masks[i] applies to modules[prunable_modules[i]].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.masks: List[torch.Tensor]\n",
    "        self.prunable_modules: List[torch.nn.Module]\n",
    "\n",
    "    def forward(self, X: Dict) -> torch.Tensor:\n",
    "        \"\"\" Forward pass of the model\n",
    "            Args:\n",
    "             - X: dictionary of batched tokenized documents (so that bert(**X) works);\n",
    "            Returns: class logits\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"override 'forward' method\")\n",
    "\n",
    "    def linearize(self, **kwargs) -> torch.nn.Sequential:\n",
    "        \"\"\" Return a linearized version of self, i.e. strip off all\n",
    "            layer/batch-norm layers, replace any MaxPooling layers with\n",
    "            AveragePooling, and remove all activations. This is required\n",
    "            for effective sparsity computation and SynFlow pruning.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"override 'linearize' method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8ba042c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearizedEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, bert_embeddings):\n",
    "        super().__init__()\n",
    "        self.shapes = {1: bert_embeddings.word_embeddings.weight.data.size(),\n",
    "                2: bert_embeddings.position_embeddings.weight.data.size(),\n",
    "                3: bert_embeddings.token_type_embeddings.weight.data.size()}\n",
    "        self.total_length = sum(shape[0] for shape in self.shapes.values())\n",
    "        bert_word_embeddings = bert_embeddings.word_embeddings.weight.data.detach().clone().t()\n",
    "        self.word_embeddings = torch.nn.Linear(self.shapes[1][0],\n",
    "                self.shapes[1][1], bias=False)\n",
    "        self.word_embeddings.weight.data = bert_word_embeddings.requires_grad_(True)\n",
    "        bert_position_embeddings = bert_embeddings.position_embeddings.weight.data.detach().clone().t()\n",
    "        self.position_embeddings = torch.nn.Linear(self.shapes[2][0],\n",
    "                self.shapes[2][1], bias=False)\n",
    "        self.position_embeddings.weight.data = bert_position_embeddings.requires_grad_(True)\n",
    "        bert_token_type_embeddings = bert_embeddings.token_type_embeddings.weight.data.detach().clone().t()\n",
    "        self.token_type_embeddings = torch.nn.Linear(self.shapes[3][0],\n",
    "                self.shapes[3][1], bias=False)\n",
    "        self.token_type_embeddings.weight.data = bert_token_type_embeddings.requires_grad_(True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = torch.squeeze(X)\n",
    "        X1 = X[:, sum(self.shapes[i][0] for i in range(1,1)):sum(self.shapes[i][0] for i in range(1,2))]\n",
    "        X2 = X[:, sum(self.shapes[i][0] for i in range(1,2)):sum(self.shapes[i][0] for i in range(1,3))]\n",
    "        X3 = X[:, sum(self.shapes[i][0] for i in range(1,3)):sum(self.shapes[i][0] for i in range(1,4))]\n",
    "        word_out = self.word_embeddings(X1)\n",
    "        position_out = self.position_embeddings(X2)\n",
    "        token_type_out = self.token_type_embeddings(X3)\n",
    "        return word_out + position_out + token_type_out\n",
    "\n",
    "\n",
    "class LinearizedBERTClassifier(ClassificationModelBase):\n",
    "    def __init__(self, reference_model, stabilize=False):\n",
    "        super().__init__()\n",
    "        self.device = reference_model.device\n",
    "        configuration = BertConfig(hidden_act=lambda x: x,\n",
    "                hidden_dropout_prob=0,\n",
    "                attention_probs_dropout_prob=0)\n",
    "        linearized_encoder = BertModel(configuration).encoder\n",
    "        if stabilize:\n",
    "            linearized_encoder.forward = stabilized_forward(\n",
    "                linearized_encoder,\n",
    "                min_=1e-2,\n",
    "                max_=1e5)\n",
    "        identity = torch.nn.Identity()\n",
    "        weights = []\n",
    "        for m in reference_model.modules_dict.encoder.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                weights.append(m.weight.data.detach().clone())\n",
    "        idx = 0\n",
    "        for m in linearized_encoder.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                m.weight.data = weights[idx].requires_grad_(True)\n",
    "                idx+=1\n",
    "            if isinstance(m, torch.nn.LayerNorm):\n",
    "                m.forward = identity.forward\n",
    "        linearized_embeddings = LinearizedEmbeddings(reference_model.modules_dict.embeddings)\n",
    "        self.total_length = linearized_embeddings.total_length\n",
    "        self.modules_dict = torch.nn.ModuleDict({\n",
    "                'embeddings': linearized_embeddings.to(self.device),\n",
    "                'encoder': linearized_encoder.to(self.device),\n",
    "                'classifier': deepcopy(reference_model.modules_dict.classifier).to(self.device)})\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                m.weight.data = torch.abs(m.weight.data)\n",
    "                m.weight.grad = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        attention_mask = torch.ones(512)[None, None, None, :]\n",
    "        x = self.modules_dict['embeddings'](inp)[None, :, :]\n",
    "        x = self.modules_dict['encoder'](x, attention_mask=attention_mask)\n",
    "        mask = {'attention_mask': torch.squeeze(attention_mask)}\n",
    "        x = {'last_hidden_state': x}\n",
    "        x = pool(x, mask, 'mean')\n",
    "        x = self.modules_dict['classifier'](x)\n",
    "        return x\n",
    "\n",
    "    def get_effective_masks(self):\n",
    "        full_length = self.total_length\n",
    "        X = torch.ones((512,full_length))\n",
    "        output = torch.sum(self(X))\n",
    "        output.backward()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Embedding):\n",
    "                print(m, m.weight.grad.reshape(-1)[:5])\n",
    "\n",
    "\n",
    "class BERTClassifier(ClassificationModelBase):\n",
    "    def __init__(self, backbone_name,\n",
    "                    pool_type,\n",
    "                    num_classes,\n",
    "                    device,\n",
    "                    **kwargs):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "        bert = BertModel.from_pretrained(backbone_name)\n",
    "        classifier = torch.nn.Linear(768, num_classes, bias=True)\n",
    "        self.modules_dict = torch.nn.ModuleDict({\n",
    "                'embeddings': bert.embeddings.to(device),\n",
    "                'encoder': bert.encoder.to(device),\n",
    "                'classifier': classifier.to(device)})\n",
    "        self.prunable_modules = {torch.nn.Embedding, torch.nn.Linear}\n",
    "        self.create_masks()\n",
    "        self.device = device\n",
    "        \n",
    "    def create_masks(self):\n",
    "        self.masks = {}\n",
    "        for stage in self.modules_dict.keys():\n",
    "            self.masks[stage] = []\n",
    "            for m in self.modules_dict[stage].modules():\n",
    "                if type(m) in self.prunable_modules:\n",
    "                    self.masks[stage].append(torch.ones(m.weight.data.size()))\n",
    "\n",
    "    def apply_masks(self):\n",
    "        for stage in self.masks.keys():\n",
    "            current_idx = 0\n",
    "            for m in self.modules_dict[stage].modules():\n",
    "                if type(m) in self.prunable_modules:\n",
    "                    m.weight.data = torch.mul(m.weight.data, self.masks[stage][current_idx])\n",
    "                    current_idx+=1\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        self.apply_masks()\n",
    "        attention_mask = inp['attention_mask'][:, None, None, :]\n",
    "        x = self.modules_dict['embeddings'](inp['input_ids'])\n",
    "        x = self.modules_dict['encoder'](x, attention_mask=attention_mask)\n",
    "        x = pool(x, inp, self.pool_type)\n",
    "        x = self.modules_dict['classifier'](x)\n",
    "        return x\n",
    "\n",
    "    def linearize(self):\n",
    "        self.apply_masks()\n",
    "        return LinearizedBERTClassifier(self, stabilize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6bd06e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = IMDb(torch.device('cpu'), 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "15146484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=30522, out_features=768, bias=False) tensor([0.0460, 0.0460, 0.0460, 0.0460, 0.0460])\n",
      "Linear(in_features=512, out_features=768, bias=False) tensor([0.0460, 0.0460, 0.0460, 0.0460, 0.0460])\n",
      "Linear(in_features=2, out_features=768, bias=False) tensor([0.0460, 0.0460, 0.0720, 0.0720, 0.0443])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0026, 0.0033, 0.0032, 0.0035, 0.0024])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0030, 0.0038, 0.0037, 0.0041, 0.0028])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.7465, 0.9359, 0.9029, 1.0032, 0.6866])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.0386, 0.9536, 0.9537, 0.9463, 0.9950])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.7595, 0.9522, 0.9187, 1.0206, 0.6986])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.2644, 1.2969, 1.3769, 1.3808, 1.1804])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0010, 0.0013, 0.0012, 0.0014, 0.0009])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0010, 0.0013, 0.0012, 0.0014, 0.0009])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.6119, 0.7669, 0.7399, 0.8219, 0.5630])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.9669, 1.0197, 1.0102, 0.9677, 0.9332])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.9334, 1.1697, 1.1286, 1.2536, 0.8588])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.4865, 1.3696, 1.3595, 1.1547, 1.2718])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0019, -0.0024, -0.0023, -0.0026, -0.0018])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0020, -0.0025, -0.0024, -0.0027, -0.0018])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.5019, 0.6287, 0.6067, 0.6738, 0.4619])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.7201, 0.6943, 0.7394, 0.7023, 0.6213])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.7558, 0.9468, 0.9136, 1.0147, 0.6956])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.2208, 1.3631, 1.4901, 1.3236, 1.4370])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0012, 0.0016, 0.0015, 0.0017, 0.0011])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0012, 0.0016, 0.0015, 0.0017, 0.0011])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.6132, 0.7679, 0.7411, 0.8230, 0.5646])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.0244, 0.9673, 0.9414, 0.8948, 0.8312])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.8031, 1.0057, 0.9707, 1.0779, 0.7395])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.4765, 1.3960, 1.4702, 1.3164, 1.3021])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0005, -0.0006, -0.0006, -0.0007, -0.0005])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0005, -0.0007, -0.0006, -0.0007, -0.0005])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.8085, 1.0120, 0.9771, 1.0848, 0.7447])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.2934, 1.2672, 1.2826, 1.2242, 1.2368])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.9340, 1.1692, 1.1289, 1.2533, 0.8604])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.4295, 1.6183, 1.2823, 1.1973, 1.4714])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0020, -0.0025, -0.0024, -0.0027, -0.0019])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0020, -0.0024, -0.0024, -0.0026, -0.0018])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.7443, 0.9313, 0.8995, 0.9985, 0.6858])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.1953, 1.2579, 1.2572, 1.2359, 1.1878])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.8914, 1.1153, 1.0773, 1.1958, 0.8214])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.3663, 1.9438, 1.2893, 1.5112, 1.3528])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0038, 0.0048, 0.0046, 0.0051, 0.0035])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0039, 0.0049, 0.0047, 0.0053, 0.0036])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.8669, 1.0842, 1.0477, 1.1628, 0.7991])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.3909, 1.4175, 1.4030, 1.4097, 1.3473])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.7004, 0.8760, 0.8465, 0.9395, 0.6457])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.2855, 1.5779, 1.4764, 1.3770, 1.6384])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0042, 0.0052, 0.0051, 0.0056, 0.0039])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0043, 0.0054, 0.0052, 0.0058, 0.0040])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.8019, 1.0025, 0.9692, 1.0754, 0.7394])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.2489, 1.2260, 1.2281, 1.1931, 1.2596])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.9029, 1.1287, 1.0913, 1.2109, 0.8326])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.2162, 1.5481, 1.3180, 1.2951, 1.3181])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0., 0., 0., 0., 0.])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0., 0., 0., 0., 0.])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.8002, 0.9999, 0.9672, 1.0729, 0.7381])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.2864, 1.2668, 1.2605, 1.2436, 1.2857])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.7958, 0.9942, 0.9618, 1.0669, 0.7340])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.3462, 1.4691, 1.6140, 1.5192, 1.4273])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0018, -0.0022, -0.0022, -0.0024, -0.0016])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0018, -0.0022, -0.0021, -0.0024, -0.0016])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.8059, 1.0063, 0.9738, 1.0802, 0.7435])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.1354, 1.1816, 1.1964, 1.1668, 1.1834])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.8873, 1.1080, 1.0723, 1.1893, 0.8187])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.3036, 1.2638, 1.3380, 1.3533, 1.3632])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0051, 0.0064, 0.0062, 0.0069, 0.0047])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.0050, 0.0063, 0.0061, 0.0068, 0.0047])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.7891, 0.9850, 0.9534, 1.0573, 0.7283])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.3005, 1.3631, 1.2138, 1.2588, 1.2860])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.9037, 1.1279, 1.0918, 1.2107, 0.8340])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.3659, 1.3689, 1.3590, 1.4083, 1.3686])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0028, -0.0035, -0.0034, -0.0038, -0.0026])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([-0.0029, -0.0037, -0.0035, -0.0039, -0.0027])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([0.7631, 0.9520, 0.9218, 1.0221, 0.7046])\n",
      "Linear(in_features=768, out_features=768, bias=True) tensor([1.3229, 1.5073, 1.5242, 1.5270, 1.5469])\n",
      "Linear(in_features=768, out_features=3072, bias=True) tensor([0.8759, 1.0925, 1.0579, 1.1729, 0.8087])\n",
      "Linear(in_features=3072, out_features=768, bias=True) tensor([1.4379, 1.5473, 1.4861, 1.2772, 1.2816])\n",
      "Linear(in_features=768, out_features=2, bias=True) tensor([1090.7365, 1360.0989, 1317.0819, 1460.0177, 1007.3458])\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = BERTClassifier('bert-base-uncased', 'mean', 2, torch.device('cpu'))\n",
    "linearized = model.linearize()\n",
    "dataloader = DataLoader(imdb_dataset.datasets['train'], batch_size=4, shuffle=True)\n",
    "linearized.get_effective_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7c026ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = BertConfig(hidden_act=lambda x: x,hidden_dropout_prob=0,attention_probs_dropout_prob=0)\n",
    "bert = BertModel(configuration)\n",
    "bert.encoder.layer[0].attention.self.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1216a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
