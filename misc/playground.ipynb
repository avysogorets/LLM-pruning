{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d671980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ccedf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataBase:\n",
    "    \"\"\" Data base class API: loads, preprocesses, and tokenizes datasets to \n",
    "        be ready for the BERT model.\n",
    "        Required attributes:\n",
    "         - num_classes: number of classes in a dataset\n",
    "         - datasets: dictionary with 'train' and 'val' tokenized datasets;\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.num_classes: int\n",
    "        self.datasets: Dict[str, Dataset] = {}  \n",
    "        \n",
    "        \n",
    "class IMDb(ClassificationDataBase):\n",
    "    def __init__(self, device, backbone_name, **kwargs):\n",
    "        super().__init__()\n",
    "        loading_kwargs = {'path': 'imdb', \n",
    "                          'train_filename': 'train.tsv',\n",
    "                          'dev_filename': 'dev.tsv',\n",
    "                          'header': 0,\n",
    "                          'index_col': 0}\n",
    "        self.device = device\n",
    "        self._prepare_datasets(loading_kwargs, device, backbone_name)\n",
    "    \n",
    "    def _prepare_datasets(self, loading_kwargs, device, backbone_name):\n",
    "        dataframes = load_dataframes(loading_kwargs)\n",
    "        self.num_classes = len(dataframes['train']['label'].unique())\n",
    "        data_X, data_y = {}, {}\n",
    "        for split in dataframes.keys():\n",
    "            data_X[split] = dataframes[split].drop('label', axis=1, inplace=False)\n",
    "            data_X[split] = data_X[split].values.tolist()\n",
    "            data_y[split] = dataframes[split]['label'].values\n",
    "            data_y[split] = torch.LongTensor(data_y[split]).to(device)\n",
    "            data_X[split] = get_tokens(data_X[split], backbone_name=backbone_name)\n",
    "            data_X[split] = {k: v.to(device) for k,v in data_X[split].items()}\n",
    "            self.datasets[split] = TokenizedDataset(data_X[split], data_y[split])\n",
    "        \n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "            \n",
    "    def __getitem__(self,idx):\n",
    "        return {key: self.X[key][idx] for key in self.X.keys()}, self.y[idx]\n",
    "\n",
    "\n",
    "def get_all_subclasses(cls):\n",
    "    all_subclasses = []\n",
    "    for subclass in cls.__subclasses__():\n",
    "        all_subclasses.append(subclass)\n",
    "        all_subclasses.extend(get_all_subclasses(subclass))\n",
    "    return all_subclasses\n",
    "\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def load_dataframes(loading_kwargs):\n",
    "    path = loading_kwargs['path']\n",
    "    train_filename = loading_kwargs['train_filename']\n",
    "    dev_filename = loading_kwargs['dev_filename']\n",
    "    header = loading_kwargs['header']\n",
    "    index_col = loading_kwargs['index_col']\n",
    "    assert '.' in train_filename, f\"unrecognized file format {train_filename}\"\n",
    "    extension = train_filename.split('.')[-1]\n",
    "    if extension == 'tsv':\n",
    "        delimiter = '\\t'\n",
    "    elif extension == 'csv':\n",
    "        delimiter = ','\n",
    "    else:\n",
    "        raise ValueError(f\"unrecognized file format {extension}\")\n",
    "    dataframes = {}\n",
    "    for split,split_filename in zip(['train','dev'], [train_filename, dev_filename]):\n",
    "        filename = os.path.join(path, split_filename)\n",
    "        dataframes[split] = pd.read_csv(\n",
    "                filename,\n",
    "                delimiter=delimiter,\n",
    "                header=header,\n",
    "                index_col=index_col,\n",
    "                engine=\"python\",\n",
    "                error_bad_lines=False,\n",
    "                warn_bad_lines=False)\n",
    "        new_columns = [f\"sentence_{i}\" for i in range(len(dataframes[split].columns)-1)]+[\"label\"]\n",
    "        dataframes[split].columns = new_columns\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def get_tokens(data_X: List[List[str]], backbone_name: str) -> dict:\n",
    "    tokenizer =  BertTokenizer.from_pretrained(backbone_name)\n",
    "    if len(data_X[0])==1:\n",
    "        data_X = [X[0] for X in data_X]\n",
    "    data_X = tokenizer.batch_encode_plus(\n",
    "                data_X,\n",
    "                add_special_tokens=True,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\")\n",
    "    return data_X\n",
    "    \n",
    "\n",
    "def stabilized_forward(linearized_encoder, min_, max_, scaler=1e3, max_attempts=10):\n",
    "    def forward(x, attention_mask=None):\n",
    "        for i in range(len(linearized_encoder.layer)):\n",
    "            attempts = 0\n",
    "            prelim_x = linearized_encoder.layer[i](x, attention_mask=attention_mask)[0]\n",
    "            while torch.mean(torch.abs(prelim_x))>max_ and attempts<max_attempts:\n",
    "                for m in linearized_encoder.layer[i].modules():\n",
    "                    if isinstance(m, torch.nn.Linear):\n",
    "                        m.weight.data = m.weight.data/scaler\n",
    "                query = linearized_encoder.layer[i].attention.self.query.weight.data\n",
    "                linearized_encoder.layer[i].attention.self.query.weight.data = scaler*query\n",
    "                key = linearized_encoder.layer[i].attention.self.key.weight.data\n",
    "                linearized_encoder.layer[i].attention.self.key.weight.data = scaler*key\n",
    "                prelim_x = linearized_encoder.layer[i](x, attention_mask=attention_mask)[0]\n",
    "                attempts+=1\n",
    "            while torch.mean(torch.abs(prelim_x))<min_ and attempts<max_attempts:\n",
    "                for m in linearized_encoder.layer[i].modules():\n",
    "                    if isinstance(m, torch.nn.Linear):\n",
    "                        m.weight.data = scaler*m.weight.data\n",
    "                query = linearized_encoder.layer[i].attention.self.query.weight.data\n",
    "                linearized_encoder.layer[i].attention.self.query.weight.data = query/scaler\n",
    "                key = linearized_encoder.layer[i].attention.self.key.weight.data\n",
    "                linearized_encoder.layer[i].attention.self.key.weight.data = key/scaler\n",
    "                prelim_x = linearized_encoder.layer[i](x, attention_mask=attention_mask)[0]\n",
    "                attempts+=1\n",
    "            assert attempts<max_attempts\n",
    "            x = prelim_x\n",
    "        return x\n",
    "    return forward\n",
    "\n",
    "\n",
    "def max_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded=(attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float())\n",
    "    token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "    return torch.max(token_embeddings, -2).values\n",
    "\n",
    "\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded=(attention_mask.unsqueeze(-1).float())\n",
    "    sum_embeddings=torch.sum(token_embeddings * input_mask_expanded, -2)\n",
    "    sum_mask=torch.clamp(input_mask_expanded.sum(-2), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def pool(x, encoded_input, pool_type):\n",
    "    if pool_type in ['avg', 'mean']:\n",
    "        x = mean_pooling(x['last_hidden_state'], encoded_input['attention_mask'])\n",
    "    elif pool_type == 'max':\n",
    "        x = max_pooling(x['last_hidden_state'], encoded_input['attention_mask'])\n",
    "    elif pool_type in ['cls', 'first']:\n",
    "        x = x['last_hidden_state'][:,0]\n",
    "    elif pool_type == 'pooler_output':\n",
    "        x = x['pooler_output']\n",
    "    else:\n",
    "        raise ValueError(f'unknown pool type {pool_type}.')\n",
    "    return x\n",
    "\n",
    "\n",
    "def effective_masks_dense(masks):\n",
    "    for i,mask in enumerate(masks):\n",
    "        assert len(mask.size())==2, f\"found mask of shape {mask.size()}\"\n",
    "        masks[i] = mask.T\n",
    "    units=[mask.shape[-2] for mask in masks]+[masks[-1].shape[-1]]\n",
    "    next_layer=torch.ones((units[-1],))\n",
    "    way_out=[next_layer]\n",
    "    for mask in masks[::-1]:\n",
    "        curr_mask=torch.matmul(mask,next_layer.view(len(next_layer),1))\n",
    "        next_layer=torch.sum(curr_mask,dim=1)>0\n",
    "        way_out.append(next_layer)\n",
    "    way_out=way_out[::-1]\n",
    "    prev_layer=torch.ones((units[0],))\n",
    "    way_in=[prev_layer]\n",
    "    for mask in masks:\n",
    "        curr_mask=torch.matmul(prev_layer.view(1,len(prev_layer)),mask)\n",
    "        prev_layer=torch.sum(curr_mask,dim=0)>0\n",
    "        way_in.append(prev_layer)\n",
    "    activity=[w_in*w_out for w_in,w_out in zip(way_in,way_out)]\n",
    "    effective_masks = []\n",
    "    for i,mask in enumerate(masks):\n",
    "        activity_prev = activity[i].view(len(activity[i]),1)\n",
    "        activity_next = activity[i+1].view(1,len(activity[i+1]))\n",
    "        effective_mask = mask*torch.matmul(activity_prev, activity_next)\n",
    "        effective_masks.append(effective_mask.T)\n",
    "    return effective_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c8f5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModelBase(torch.nn.Module):\n",
    "    \"\"\" The base model API\n",
    "        Required attributes:\n",
    "         - masks: dictionary storing lists of binary masks for\n",
    "           each stage ('embeddings', 'encoder', 'classifier')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.masks: Dict[str, List[torch.Tensor]]\n",
    "\n",
    "    def forward(self, X: Dict) -> torch.Tensor:\n",
    "        \"\"\" Forward pass of the model\n",
    "            Args:\n",
    "             - X: dictionary of batched tokenized documents\n",
    "            Returns: class logits\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"override 'forward' method\")\n",
    "\n",
    "    def effective_masks(self, **kwargs) -> Dict[str, List[torch.Tensor]]:\n",
    "        \"\"\" Return effective masks in a dictionary by module:\n",
    "            'embeddings', 'encoder', 'classifier'\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"override 'linearize' method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ba042c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearizedEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, bert_embeddings):\n",
    "        super().__init__()\n",
    "        self.shapes = {1: bert_embeddings.word_embeddings.weight.data.size(),\n",
    "                2: bert_embeddings.position_embeddings.weight.data.size(),\n",
    "                3: bert_embeddings.token_type_embeddings.weight.data.size()}\n",
    "        self.total_length = sum(shape[0] for shape in self.shapes.values())\n",
    "        bert_word_embeddings = bert_embeddings.word_embeddings.weight.data.detach().clone().t()\n",
    "        self.word_embeddings = torch.nn.Linear(self.shapes[1][0],\n",
    "                self.shapes[1][1], bias=False)\n",
    "        self.word_embeddings.weight.data = bert_word_embeddings.requires_grad_(True)\n",
    "        bert_position_embeddings = bert_embeddings.position_embeddings.weight.data.detach().clone().t()\n",
    "        self.position_embeddings = torch.nn.Linear(self.shapes[2][0],\n",
    "                self.shapes[2][1], bias=False)\n",
    "        self.position_embeddings.weight.data = bert_position_embeddings.requires_grad_(True)\n",
    "        bert_token_type_embeddings = bert_embeddings.token_type_embeddings.weight.data.detach().clone().t()\n",
    "        self.token_type_embeddings = torch.nn.Linear(self.shapes[3][0],\n",
    "                self.shapes[3][1], bias=False)\n",
    "        self.token_type_embeddings.weight.data = bert_token_type_embeddings.requires_grad_(True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = torch.squeeze(X)\n",
    "        X1 = X[:, sum(self.shapes[i][0] for i in range(1,1)):sum(self.shapes[i][0] for i in range(1,2))]\n",
    "        X2 = X[:, sum(self.shapes[i][0] for i in range(1,2)):sum(self.shapes[i][0] for i in range(1,3))]\n",
    "        X3 = X[:, sum(self.shapes[i][0] for i in range(1,3)):sum(self.shapes[i][0] for i in range(1,4))]\n",
    "        word_out = self.word_embeddings(X1)\n",
    "        position_out = self.position_embeddings(X2)\n",
    "        token_type_out = self.token_type_embeddings(X3)\n",
    "        return word_out + position_out + token_type_out\n",
    "\n",
    "\n",
    "class LinearizedBERTClassifier(ClassificationModelBase):\n",
    "    def __init__(self, reference_model, stabilize=False):\n",
    "        super().__init__()\n",
    "        self.device = reference_model.device\n",
    "        self.stabilize = stabilize\n",
    "        configuration = BertConfig(hidden_act=lambda x: x,\n",
    "                hidden_dropout_prob=0,\n",
    "                attention_probs_dropout_prob=0)\n",
    "        linearized_encoder = BertModel(configuration).encoder\n",
    "        if stabilize:\n",
    "            linearized_encoder.forward = stabilized_forward(\n",
    "                linearized_encoder,\n",
    "                min_=1e-2,\n",
    "                max_=1e5)\n",
    "        identity = torch.nn.Identity()\n",
    "        weights = []\n",
    "        for m in reference_model.modules_dict.encoder.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                weights.append(m.weight.data.detach().clone())\n",
    "        idx = 0\n",
    "        for m in linearized_encoder.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                m.weight.data = weights[idx].requires_grad_(True)\n",
    "                idx+=1\n",
    "            if isinstance(m, torch.nn.LayerNorm):\n",
    "                m.forward = identity.forward\n",
    "        linearized_embeddings = LinearizedEmbeddings(reference_model.modules_dict.embeddings)\n",
    "        self.total_length = linearized_embeddings.total_length\n",
    "        self.modules_dict = torch.nn.ModuleDict({\n",
    "                'embeddings': linearized_embeddings.to(self.device),\n",
    "                'encoder': linearized_encoder.to(self.device),\n",
    "                #'classifier': deepcopy(reference_model.modules_dict.classifier).to(self.device)\n",
    "        })\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                m.weight.data = torch.abs(m.weight.data)\n",
    "                m.weight.grad = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        attention_mask = torch.ones(512)[None, None, None, :]\n",
    "        x = self.modules_dict['embeddings'](inp)[None, :, :]\n",
    "        x = self.modules_dict['encoder'](x, attention_mask=attention_mask)\n",
    "        mask = {'attention_mask': torch.squeeze(attention_mask)}\n",
    "        if self.stabilize:\n",
    "            x = {'last_hidden_state': x}\n",
    "        x = pool(x, mask, 'mean')\n",
    "        #x = self.modules_dict['classifier'](x)\n",
    "        return x\n",
    "\n",
    "    def get_effective_masks(self):\n",
    "        effective_masks = {}\n",
    "        full_length = self.total_length\n",
    "        X = torch.ones((512,full_length))\n",
    "        output = torch.sum(self(X))\n",
    "        output.backward()\n",
    "        for stage in self.modules_dict.keys():\n",
    "            effective_masks[stage] = []\n",
    "            for m in self.modules_dict[stage].modules():\n",
    "                if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Embedding):\n",
    "                    print(m.weight.grad.size(), m.weight.grad.reshape(-1)[:5], torch.min(m.weight.grad), torch.max(m.weight.grad))\n",
    "                    effective_masks[stage].append((torch.abs(m.weight.grad)>0).int())\n",
    "                    m.weight.grad = None\n",
    "        return effective_masks\n",
    "\n",
    "\n",
    "class BERTClassifier(ClassificationModelBase):\n",
    "    def __init__(self, backbone_name,\n",
    "                    pool_type,\n",
    "                    num_classes,\n",
    "                    device,\n",
    "                    **kwargs):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "        bert = BertModel.from_pretrained(backbone_name)\n",
    "        classifier = torch.nn.Linear(768, num_classes, bias=True)\n",
    "        self.modules_dict = torch.nn.ModuleDict({\n",
    "                'embeddings': bert.embeddings.to(device),\n",
    "                'encoder': bert.encoder.to(device),\n",
    "                #'classifier': classifier.to(device)\n",
    "        })\n",
    "        self.prunable_modules = {torch.nn.Embedding, torch.nn.Linear}\n",
    "        self.create_masks()\n",
    "        self.device = device\n",
    "        self.effective_masks\n",
    "        \n",
    "    def create_masks(self):\n",
    "        self.masks = {}\n",
    "        for stage in self.modules_dict.keys():\n",
    "            self.masks[stage] = []\n",
    "            for m in self.modules_dict[stage].modules():\n",
    "                if type(m) in self.prunable_modules:\n",
    "                    self.masks[stage].append(torch.ones(m.weight.data.size()))\n",
    "\n",
    "    def apply_masks(self):\n",
    "        for stage in self.masks.keys():\n",
    "            current_idx = 0\n",
    "            for m in self.modules_dict[stage].modules():\n",
    "                if type(m) in self.prunable_modules:\n",
    "                    m.weight.data = torch.mul(m.weight.data, self.masks[stage][current_idx])\n",
    "                    current_idx+=1\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        self.apply_masks()\n",
    "        attention_mask = inp['attention_mask'][:, None, None, :]\n",
    "        x = self.modules_dict['embeddings'](inp['input_ids'])\n",
    "        x = self.modules_dict['encoder'](x, attention_mask=attention_mask)\n",
    "        x = pool(x, inp, self.pool_type)\n",
    "        #x = self.modules_dict['classifier'](x)\n",
    "        return x\n",
    "\n",
    "    def linearize(self):\n",
    "        self.apply_masks()\n",
    "        return LinearizedBERTClassifier(self, stabilize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71d34c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=10, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.nn.Linear(10,10)\n",
    "list(iter(a.modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd06e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = IMDb(torch.device('cpu'), 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84392362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 30522]) tensor([0.0332, 0.0332, 0.0332, 0.0332, 0.0332]) tensor(-0.4529) tensor(0.5298)\n",
      "torch.Size([768, 512]) tensor([0.0332, 0.0332, 0.0332, 0.0332, 0.0332]) tensor(-0.4529) tensor(0.5298)\n",
      "torch.Size([768, 2]) tensor([0.0332, 0.0332, 0.0658, 0.0658, 0.0956]) tensor(-0.4529) tensor(0.5298)\n",
      "torch.Size([768, 768]) tensor([42.2640, 52.9883, 51.1211, 56.7959, 38.8760]) tensor(-238.6008) tensor(192.4649)\n",
      "torch.Size([768, 768]) tensor([48.2956, 60.5500, 58.4161, 64.9012, 44.4238]) tensor(-239.9757) tensor(238.0393)\n",
      "torch.Size([768, 768]) tensor([14.6375, 18.3516, 17.7049, 19.6704, 13.4641]) tensor(7.3030) tensor(67.6220)\n",
      "torch.Size([768, 768]) tensor([15.0646, 13.8326, 13.8340, 13.7263, 14.4319]) tensor(5.4796) tensor(36.2494)\n",
      "torch.Size([3072, 768]) tensor([14.2495, 17.8647, 17.2354, 19.1481, 13.1073]) tensor(8.5118) tensor(124.5001)\n",
      "torch.Size([768, 3072]) tensor([18.3313, 18.8038, 19.9627, 20.0192, 17.1135]) tensor(5.6353) tensor(81.2385)\n",
      "torch.Size([768, 768]) tensor([ -8.1089, -10.1621,  -9.8047, -10.8911,  -7.4608]) tensor(-51.7744) tensor(136.8868)\n",
      "torch.Size([768, 768]) tensor([ -8.0522, -10.0910,  -9.7361, -10.8149,  -7.4086]) tensor(-53.1208) tensor(136.0155)\n",
      "torch.Size([768, 768]) tensor([5.3999, 6.7672, 6.5293, 7.2527, 4.9684]) tensor(3.0898) tensor(35.0655)\n",
      "torch.Size([768, 768]) tensor([6.2810, 6.6240, 6.5622, 6.2859, 6.0619]) tensor(1.8128) tensor(25.2124)\n",
      "torch.Size([3072, 768]) tensor([ 8.0864, 10.1338,  9.7774, 10.8608,  7.4403]) tensor(3.9007) tensor(73.7754)\n",
      "torch.Size([768, 3072]) tensor([9.6502, 8.8914, 8.8261, 7.4964, 8.2566]) tensor(2.0711) tensor(48.3887)\n",
      "torch.Size([768, 768]) tensor([1.6947, 2.1231, 2.0487, 2.2753, 1.5598]) tensor(-36.1129) tensor(48.6252)\n",
      "torch.Size([768, 768]) tensor([1.7855, 2.2369, 2.1584, 2.3972, 1.6434]) tensor(-36.6146) tensor(49.7304)\n",
      "torch.Size([768, 768]) tensor([3.2554, 4.0783, 3.9353, 4.3706, 2.9963]) tensor(2.4929) tensor(22.4844)\n",
      "torch.Size([768, 768]) tensor([3.2457, 3.1293, 3.3328, 3.1656, 2.8003]) tensor(0.9957) tensor(16.9582)\n",
      "torch.Size([3072, 768]) tensor([4.6499, 5.8252, 5.6211, 6.2426, 4.2799]) tensor(1.8892) tensor(46.1218)\n",
      "torch.Size([768, 3072]) tensor([5.4989, 6.1400, 6.7120, 5.9620, 6.4727]) tensor(1.2953) tensor(85.9929)\n",
      "torch.Size([768, 768]) tensor([-14.9570, -18.7300, -18.0778, -20.0742, -13.7713]) tensor(-119.5175) tensor(149.1952)\n",
      "torch.Size([768, 768]) tensor([-14.8763, -18.6289, -17.9802, -19.9658, -13.6970]) tensor(-120.7148) tensor(146.2305)\n",
      "torch.Size([768, 768]) tensor([ 7.7449,  9.6986,  9.3608, 10.3945,  7.1309]) tensor(5.2413) tensor(45.6013)\n",
      "torch.Size([768, 768]) tensor([10.0721,  9.5109,  9.2558,  8.7975,  8.1729]) tensor(5.3387) tensor(24.8959)\n",
      "torch.Size([3072, 768]) tensor([10.2065, 12.7808, 12.3362, 13.6978,  9.3977]) tensor(4.9111) tensor(120.1117)\n",
      "torch.Size([768, 3072]) tensor([14.5103, 13.7188, 14.4482, 12.9371, 12.7966]) tensor(5.3734) tensor(101.8654)\n",
      "torch.Size([768, 768]) tensor([-12.2978, -15.3945, -14.8633, -16.5013, -11.3274]) tensor(-118.6659) tensor(108.0445)\n",
      "torch.Size([768, 768]) tensor([-12.6328, -15.8138, -15.2681, -16.9507, -11.6359]) tensor(-115.2993) tensor(107.3360)\n",
      "torch.Size([768, 768]) tensor([4.2243, 5.2880, 5.1056, 5.6682, 3.8909]) tensor(2.0940) tensor(18.9211)\n",
      "torch.Size([768, 768]) tensor([5.4706, 5.3596, 5.4249, 5.1779, 5.2312]) tensor(1.1453) tensor(11.2271)\n",
      "torch.Size([3072, 768]) tensor([5.0645, 6.3394, 6.1212, 6.7955, 4.6651]) tensor(1.0490) tensor(40.4418)\n",
      "torch.Size([768, 3072]) tensor([6.0436, 6.8417, 5.4213, 5.0619, 6.2207]) tensor(0.9984) tensor(38.9101)\n",
      "torch.Size([768, 768]) tensor([16.1851, 20.2519, 19.5602, 21.7131, 14.9133]) tensor(-104.3881) tensor(83.3079)\n",
      "torch.Size([768, 768]) tensor([15.6179, 19.5423, 18.8749, 20.9522, 14.3909]) tensor(-107.0624) tensor(83.3830)\n",
      "torch.Size([768, 768]) tensor([5.4834, 6.8612, 6.6270, 7.3563, 5.0526]) tensor(3.2385) tensor(24.7690)\n",
      "torch.Size([768, 768]) tensor([6.5168, 6.8582, 6.8545, 6.7383, 6.4760]) tensor(2.7181) tensor(14.5857)\n",
      "torch.Size([3072, 768]) tensor([6.5165, 8.1535, 7.8756, 8.7423, 6.0049]) tensor(3.0043) tensor(62.9209)\n",
      "torch.Size([768, 3072]) tensor([ 7.4447, 10.5915,  7.0251,  8.2345,  7.3712]) tensor(2.8022) tensor(37.6361)\n",
      "torch.Size([768, 768]) tensor([-26.1695, -32.7299, -31.6264, -35.1021, -24.1234]) tensor(-97.2876) tensor(86.6439)\n",
      "torch.Size([768, 768]) tensor([-26.8887, -33.6294, -32.4956, -36.0668, -24.7863]) tensor(-96.9330) tensor(86.8496)\n",
      "torch.Size([768, 768]) tensor([4.4565, 5.5737, 5.3858, 5.9777, 4.1081]) tensor(2.1438) tensor(16.9368)\n",
      "torch.Size([768, 768]) tensor([5.2156, 5.3153, 5.2612, 5.2863, 5.0524]) tensor(2.1433) tensor(9.5556)\n",
      "torch.Size([3072, 768]) tensor([3.5853, 4.4840, 4.3331, 4.8091, 3.3052]) tensor(2.0863) tensor(49.8126)\n",
      "torch.Size([768, 3072]) tensor([4.8173, 5.9132, 5.5328, 5.1603, 6.1401]) tensor(1.9811) tensor(21.8198)\n",
      "torch.Size([768, 768]) tensor([6.5145, 8.1441, 7.8737, 8.7368, 6.0070]) tensor(-10.3681) tensor(32.8878)\n",
      "torch.Size([768, 768]) tensor([6.7357, 8.4207, 8.1411, 9.0335, 6.2109]) tensor(-10.5418) tensor(34.8981)\n",
      "torch.Size([768, 768]) tensor([1.2874, 1.6095, 1.5560, 1.7266, 1.1871]) tensor(0.6733) tensor(4.9822)\n",
      "torch.Size([768, 768]) tensor([1.3609, 1.3360, 1.3382, 1.3001, 1.3726]) tensor(0.1133) tensor(3.2803)\n",
      "torch.Size([3072, 768]) tensor([1.4756, 1.8446, 1.7835, 1.9789, 1.3607]) tensor(0.8116) tensor(13.8799)\n",
      "torch.Size([768, 3072]) tensor([1.3242, 1.6855, 1.4350, 1.4101, 1.4351]) tensor(0.1030) tensor(8.7672)\n",
      "torch.Size([768, 768]) tensor([-1.1584, -1.4474, -1.4001, -1.5532, -1.0684]) tensor(-31.6103) tensor(33.8101)\n",
      "torch.Size([768, 768]) tensor([-1.1173, -1.3960, -1.3503, -1.4980, -1.0305]) tensor(-31.6686) tensor(34.5234)\n",
      "torch.Size([768, 768]) tensor([1.2963, 1.6197, 1.5667, 1.7380, 1.1956]) tensor(0.8951) tensor(5.5800)\n",
      "torch.Size([768, 768]) tensor([1.5532, 1.5294, 1.5219, 1.5015, 1.5523]) tensor(0.3318) tensor(3.4089)\n",
      "torch.Size([3072, 768]) tensor([1.3511, 1.6880, 1.6329, 1.8115, 1.2462]) tensor(0.8096) tensor(10.0472)\n",
      "torch.Size([768, 3072]) tensor([1.6243, 1.7727, 1.9474, 1.8331, 1.7223]) tensor(0.2647) tensor(10.4467)\n",
      "torch.Size([768, 768]) tensor([2.3740, 2.9645, 2.8687, 3.1820, 2.1902]) tensor(-12.7017) tensor(14.3591)\n",
      "torch.Size([768, 768]) tensor([2.3431, 2.9260, 2.8314, 3.1407, 2.1617]) tensor(-12.3878) tensor(14.1238)\n",
      "torch.Size([768, 768]) tensor([0.6923, 0.8645, 0.8366, 0.9280, 0.6387]) tensor(0.4592) tensor(3.0753)\n",
      "torch.Size([768, 768]) tensor([0.6092, 0.6340, 0.6419, 0.6260, 0.6349]) tensor(-0.8200) tensor(2.5719)\n",
      "torch.Size([3072, 768]) tensor([0.7549, 0.9426, 0.9122, 1.0118, 0.6965]) tensor(0.3261) tensor(10.1209)\n",
      "torch.Size([768, 3072]) tensor([0.6987, 0.6774, 0.7171, 0.7254, 0.7307]) tensor(-4.3253) tensor(13.5283)\n",
      "torch.Size([768, 768]) tensor([2.2774, 2.8426, 2.7517, 3.0515, 2.1018]) tensor(-5.2607) tensor(8.0493)\n",
      "torch.Size([768, 768]) tensor([2.2433, 2.8001, 2.7105, 3.0058, 2.0704]) tensor(-5.3753) tensor(8.2428)\n",
      "torch.Size([768, 768]) tensor([0.4104, 0.5123, 0.4959, 0.5499, 0.3788]) tensor(0.2105) tensor(1.9975)\n",
      "torch.Size([768, 768]) tensor([0.3602, 0.3775, 0.3361, 0.3486, 0.3561]) tensor(-1.7738) tensor(2.2172)\n",
      "torch.Size([3072, 768]) tensor([0.4936, 0.6161, 0.5964, 0.6613, 0.4556]) tensor(-1.0182) tensor(2.8082)\n",
      "torch.Size([768, 3072]) tensor([0.3777, 0.3785, 0.3758, 0.3894, 0.3784]) tensor(-7.3272) tensor(9.1448)\n",
      "torch.Size([768, 768]) tensor([-5.6541, -7.0537, -6.8296, -7.5726, -5.2201]) tensor(-27.5029) tensor(28.6787)\n",
      "torch.Size([768, 768]) tensor([-5.8597, -7.3102, -7.0780, -7.8480, -5.4099]) tensor(-28.2689) tensor(28.8974)\n",
      "torch.Size([768, 768]) tensor([0.7669, 0.9567, 0.9263, 1.0271, 0.7080]) tensor(0.5860) tensor(3.6058)\n",
      "torch.Size([768, 768]) tensor([0.7889, 0.8989, 0.9089, 0.9106, 0.9225]) tensor(0.0508) tensor(2.5588)\n",
      "torch.Size([3072, 768]) tensor([0.8537, 1.0649, 1.0311, 1.1431, 0.7882]) tensor(0.4250) tensor(6.9479)\n",
      "torch.Size([768, 3072]) tensor([0.8565, 0.9217, 0.8852, 0.7608, 0.7634]) tensor(0.0343) tensor(4.9473)\n",
      "torch.Size([2, 768]) tensor([1090.7365, 1360.0989, 1317.0819, 1460.0177, 1007.3458]) tensor(915.8544) tensor(3782.5544)\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = BERTClassifier('bert-base-uncased', 'mean', 2, torch.device('cpu'))\n",
    "linearized = model.linearize()\n",
    "dataloader = DataLoader(imdb_dataset.datasets['train'], batch_size=4, shuffle=True)\n",
    "effective_masks = linearized.get_effective_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7c026ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = BertConfig(hidden_act=lambda x: x,hidden_dropout_prob=0,attention_probs_dropout_prob=0)\n",
    "bert = BertModel(configuration)\n",
    "bert.encoder.layer[0].attention.self.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "186c939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) False\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([768, 3072]) True\n"
     ]
    }
   ],
   "source": [
    "for mask in effective_masks['encoder']:\n",
    "    print(mask.shape, (mask.numpy()>0).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20335d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
